{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using YAMNet to Classify Environmental Sounds\n",
    "\n",
    "In this notebook, we'll use the YAMNet model to classify environmental sounds into a set of pre-defined categories. We'll start by training the model on the [ESC-50 dataset](https://github.com/karoldvl/ESC-50), which consists of 2000 environmental recordings from 50 classes. Then, we'll use the model to classify some new sounds.\n",
    "\n",
    "The YAMNet model is a pre-trained deep convolutional neural network that was trained to classify environmental sounds. It was trained on the [AudioSet dataset](https://research.google.com/audioset/) and can classify sounds into 521 different categories.\n",
    "\n",
    "We'll use TensorFlow Hub to download the pre-trained YAMNet model, and TensorFlow to train the final layer to classify the chosen classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n",
    "Before we get started, let's install and import the necessary libraries and modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install numpy tensorflow tensorflow-hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.6.0'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "import tensorflow_hub as hub\n",
    "import numpy as np\n",
    "\n",
    "# Print the version of TensorFlow\n",
    "print(tf.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load the ESC-50 Dataset\n",
    "\n",
    "The ESC-50 dataset consists of 2000 environmental recordings from 50 classes. Each recording is 5 seconds long and has a sample rate of 44.1 kHz.\n",
    "\n",
    "We'll use TensorFlow's `tf.data` API to load the dataset and preprocess the audio files. Specifically, we'll do the following:\n",
    "\n",
    "- Load the audio file and resample it to 16 kHz.\n",
    "- Extract mel spectrograms using a window size of 0.025 seconds and a hop size of 0.010 seconds.\n",
    "- Normalize the mel spectrograms by subtracting the mean and dividing by the standard deviation of the training set.\n",
    "- Split the dataset into training, validation, and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetV1Adapter(<PrefetchDataset shapes: ((?, 128, 64,
