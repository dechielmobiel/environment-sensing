{
  "metadata": {
    "language_info": {
      "codemirror_mode": {
        "name": "python",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8"
    },
    "kernelspec": {
      "name": "python",
      "display_name": "Python (Pyodide)",
      "language": "python"
    }
  },
  "nbformat_minor": 4,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "code",
      "source": "pip install -q \"tensorflow==2.11.*\"\n# tensorflow_io 0.28 is compatible with TensorFlow 2.11\npip install -q \"tensorflow_io==0.28.*\"",
      "metadata": {
        "trusted": true
      },
      "execution_count": 3,
      "outputs": [
        {
          "ename": "<class 'SyntaxError'>",
          "evalue": "invalid syntax (<ipython-input-3-a70e6b4c66cf>, line 1)",
          "traceback": [
            "\u001b[0;36m  Cell \u001b[0;32mIn[3], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    pip install -q \"tensorflow==2.11.*\"\u001b[0m\n\u001b[0m        ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "import os\n\nfrom IPython import display\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport pandas as pd\n\nimport tensorflow as tf\nimport tensorflow_hub as hub\nimport tensorflow_io as tfio",
      "metadata": {
        "trusted": true
      },
      "execution_count": 2,
      "outputs": [
        {
          "ename": "<class 'ModuleNotFoundError'>",
          "evalue": "No module named 'tensorflow'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
            "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mnumpy\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mnp\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mpandas\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mpd\u001b[39;00m\n\u001b[0;32m----> 8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtf\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_hub\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mhub\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_io\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfio\u001b[39;00m\n",
            "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow'"
          ],
          "output_type": "error"
        }
      ]
    },
    {
      "cell_type": "code",
      "source": "yamnet_model_handle = 'https://tfhub.dev/google/yamnet/1'\nyamnet_model = hub.load(yamnet_model_handle)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "testing_wav_file_name = tf.keras.utils.get_file('miaow_16k.wav',\n                                                'https://storage.googleapis.com/audioset/miaow_16k.wav',\n                                                cache_dir='./',\n                                                cache_subdir='test_data')\n\nprint(testing_wav_file_name)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Utility functions for loading audio files and making sure the sample rate is correct.\n\n@tf.function\ndef load_wav_16k_mono(filename):\n    \"\"\" Load a WAV file, convert it to a float tensor, resample to 16 kHz single-channel audio. \"\"\"\n    file_contents = tf.io.read_file(filename)\n    wav, sample_rate = tf.audio.decode_wav(\n          file_contents,\n          desired_channels=1)\n    wav = tf.squeeze(wav, axis=-1)\n    sample_rate = tf.cast(sample_rate, dtype=tf.int64)\n    wav = tfio.audio.resample(wav, rate_in=sample_rate, rate_out=16000)\n    return wav",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "testing_wav_data = load_wav_16k_mono(testing_wav_file_name)\n\n_ = plt.plot(testing_wav_data)\n\n# Play the audio file.\ndisplay.Audio(testing_wav_data, rate=16000)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "class_map_path = yamnet_model.class_map_path().numpy().decode('utf-8')\nclass_names =list(pd.read_csv(class_map_path)['display_name'])\n\nfor name in class_names[:20]:\n  print(name)\nprint('...')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "scores, embeddings, spectrogram = yamnet_model(testing_wav_data)\nclass_scores = tf.reduce_mean(scores, axis=0)\ntop_class = tf.math.argmax(class_scores)\ninferred_class = class_names[top_class]\n\nprint(f'The main sound is: {inferred_class}')\nprint(f'The embeddings shape: {embeddings.shape}')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "_ = tf.keras.utils.get_file('esc-50.zip',\n                        'https://github.com/karoldvl/ESC-50/archive/master.zip',\n                        cache_dir='./',\n                        cache_subdir='datasets',\n                        extract=True)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "esc50_csv = './datasets/ESC-50-master/meta/esc50.csv'\nbase_data_path = './datasets/ESC-50-master/audio/'\n\npd_data = pd.read_csv(esc50_csv)\npd_data.head()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "my_classes = ['dog', 'cat']\nmap_class_to_id = {'dog':0, 'cat':1}\n\nfiltered_pd = pd_data[pd_data.category.isin(my_classes)]\n\nclass_id = filtered_pd['category'].apply(lambda name: map_class_to_id[name])\nfiltered_pd = filtered_pd.assign(target=class_id)\n\nfull_path = filtered_pd['filename'].apply(lambda row: os.path.join(base_data_path, row))\nfiltered_pd = filtered_pd.assign(filename=full_path)\n\nfiltered_pd.head(10)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "filenames = filtered_pd['filename']\ntargets = filtered_pd['target']\nfolds = filtered_pd['fold']\n\nmain_ds = tf.data.Dataset.from_tensor_slices((filenames, targets, folds))\nmain_ds.element_spec",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "def load_wav_for_map(filename, label, fold):\n  return load_wav_16k_mono(filename), label, fold\n\nmain_ds = main_ds.map(load_wav_for_map)\nmain_ds.element_spec",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# applies the embedding extraction model to a wav data\ndef extract_embedding(wav_data, label, fold):\n  ''' run YAMNet to extract embedding from the wav data '''\n  scores, embeddings, spectrogram = yamnet_model(wav_data)\n  num_embeddings = tf.shape(embeddings)[0]\n  return (embeddings,\n            tf.repeat(label, num_embeddings),\n            tf.repeat(fold, num_embeddings))\n\n# extract embedding\nmain_ds = main_ds.map(extract_embedding).unbatch()\nmain_ds.element_spec",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "cached_ds = main_ds.cache()\ntrain_ds = cached_ds.filter(lambda embedding, label, fold: fold < 4)\nval_ds = cached_ds.filter(lambda embedding, label, fold: fold == 4)\ntest_ds = cached_ds.filter(lambda embedding, label, fold: fold == 5)\n\n# remove the folds column now that it's not needed anymore\nremove_fold_column = lambda embedding, label, fold: (embedding, label)\n\ntrain_ds = train_ds.map(remove_fold_column)\nval_ds = val_ds.map(remove_fold_column)\ntest_ds = test_ds.map(remove_fold_column)\n\ntrain_ds = train_ds.cache().shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)\nval_ds = val_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)\ntest_ds = test_ds.cache().batch(32).prefetch(tf.data.AUTOTUNE)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "my_model = tf.keras.Sequential([\n    tf.keras.layers.Input(shape=(1024), dtype=tf.float32,\n                          name='input_embedding'),\n    tf.keras.layers.Dense(512, activation='relu'),\n    tf.keras.layers.Dense(len(my_classes))\n], name='my_model')\n\nmy_model.summary()",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "my_model.compile(loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True),\n                 optimizer=\"adam\",\n                 metrics=['accuracy'])\n\ncallback = tf.keras.callbacks.EarlyStopping(monitor='loss',\n                                            patience=3,\n                                            restore_best_weights=True)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "history = my_model.fit(train_ds,\n                       epochs=20,\n                       validation_data=val_ds,\n                       callbacks=callback)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "loss, accuracy = my_model.evaluate(test_ds)\n\nprint(\"Loss: \", loss)\nprint(\"Accuracy: \", accuracy)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "scores, embeddings, spectrogram = yamnet_model(testing_wav_data)\nresult = my_model(embeddings).numpy()\n\ninferred_class = my_classes[result.mean(axis=0).argmax()]\nprint(f'The main sound is: {inferred_class}')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "class ReduceMeanLayer(tf.keras.layers.Layer):\n  def __init__(self, axis=0, **kwargs):\n    super(ReduceMeanLayer, self).__init__(**kwargs)\n    self.axis = axis\n\n  def call(self, input):\n    return tf.math.reduce_mean(input, axis=self.axis)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "saved_model_path = './dogs_and_cats_yamnet'\n\ninput_segment = tf.keras.layers.Input(shape=(), dtype=tf.float32, name='audio')\nembedding_extraction_layer = hub.KerasLayer(yamnet_model_handle,\n                                            trainable=False, name='yamnet')\n_, embeddings_output, _ = embedding_extraction_layer(input_segment)\nserving_outputs = my_model(embeddings_output)\nserving_outputs = ReduceMeanLayer(axis=0, name='classifier')(serving_outputs)\nserving_model = tf.keras.Model(input_segment, serving_outputs)\nserving_model.save(saved_model_path, include_optimizer=False)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "reloaded_model = tf.saved_model.load(saved_model_path)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "reloaded_results = reloaded_model(testing_wav_data)\ncat_or_dog = my_classes[tf.math.argmax(reloaded_results)]\nprint(f'The main sound is: {cat_or_dog}')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "serving_results = reloaded_model.signatures['serving_default'](testing_wav_data)\ncat_or_dog = my_classes[tf.math.argmax(serving_results['classifier'])]\nprint(f'The main sound is: {cat_or_dog}')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "test_pd = filtered_pd.loc[filtered_pd['fold'] == 5]\nrow = test_pd.sample(1)\nfilename = row['filename'].item()\nprint(filename)\nwaveform = load_wav_16k_mono(filename)\nprint(f'Waveform values: {waveform}')\n_ = plt.plot(waveform)\n\ndisplay.Audio(waveform, rate=16000)",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": "# Run the model, check the output.\nscores, embeddings, spectrogram = yamnet_model(waveform)\nclass_scores = tf.reduce_mean(scores, axis=0)\ntop_class = tf.math.argmax(class_scores)\ninferred_class = class_names[top_class]\ntop_score = class_scores[top_class]\nprint(f'[YAMNet] The main sound is: {inferred_class} ({top_score})')\n\nreloaded_results = reloaded_model(waveform)\nyour_top_class = tf.math.argmax(reloaded_results)\nyour_inferred_class = my_classes[your_top_class]\nclass_probabilities = tf.nn.softmax(reloaded_results, axis=-1)\nyour_top_score = class_probabilities[your_top_class]\nprint(f'[Your model] The main sound is: {your_inferred_class} ({your_top_score})')",
      "metadata": {},
      "execution_count": null,
      "outputs": []
    }
  ]
}